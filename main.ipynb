{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.data import Data\n",
    "\n",
    "dataset_name = \"one_track\"\n",
    "\n",
    "graph_df = pd.read_csv(f\"out/{dataset_name}.csv\")\n",
    "sources = graph_df.u.values\n",
    "destinations = graph_df.i.values\n",
    "edge_idxs = graph_df.idx.values\n",
    "labels = graph_df.label.values\n",
    "timestamps = graph_df.ts.values\n",
    "\n",
    "data = Data(sources, destinations, timestamps, edge_idxs, labels)\n",
    "\n",
    "from utils import RandEdgeSampler, get_neighbor_finder\n",
    "\n",
    "neighbor_finder = get_neighbor_finder(data, uniform=False)\n",
    "random_sampler = RandEdgeSampler(sources, destinations)\n",
    "size = len(sources)\n",
    "_, negatives = random_sampler.sample(size)\n",
    "print(size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:03:44.373919Z",
     "start_time": "2024-03-15T02:03:42.405066Z"
    }
   },
   "id": "1078b0040abfba6d",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "TGN(\n  (time_encoder): TimeEncoder(\n    (w): Linear(in_features=1, out_features=172, bias=True)\n  )\n  (memory): Memory()\n  (message_aggregator): LastMessageAggregator()\n  (message_function): IdentityMessageFunction()\n  (memory_updater): GRUMemoryUpdater(\n    (memory): Memory()\n    (layer_norm): LayerNorm((172,), eps=1e-05, elementwise_affine=True)\n    (memory_updater): GRUCell(517, 172)\n  )\n  (embedding_module): AttentionEmbedding(\n    (time_encoder): TimeEncoder(\n      (w): Linear(in_features=1, out_features=172, bias=True)\n    )\n    (attention_models): ModuleList(\n      (0): TemporalAttentionLayer(\n        (merger): MergeLayer(\n          (fc1): Linear(in_features=516, out_features=172, bias=True)\n          (fc2): Linear(in_features=172, out_features=172, bias=True)\n          (activation): ReLU()\n        )\n        (multi_head_target): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=344, out_features=344, bias=True)\n        )\n      )\n    )\n  )\n  (affinity_score): MergeLayer(\n    (fc1): Linear(in_features=344, out_features=172, bias=True)\n    (fc2): Linear(in_features=172, out_features=1, bias=True)\n    (activation): ReLU()\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models.tgn import TGN\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "node_features = np.load(f\"out/{dataset_name}_node.npy\")\n",
    "edge_features = np.load(f\"out/{dataset_name}_edge.npy\")\n",
    "\n",
    "tgn = TGN(neighbor_finder=neighbor_finder, node_features=node_features,\n",
    "          edge_features=edge_features, device=device,\n",
    "          n_layers=1, n_heads=2, dropout=0.1, use_memory=True,\n",
    "          message_dimension=100, memory_dimension=172,\n",
    "          memory_update_at_start=False,\n",
    "          embedding_module_type=\"attention\",\n",
    "          message_function=\"identity\",\n",
    "          aggregator_type=\"last\",\n",
    "          memory_updater_type=\"gru\",\n",
    "          n_neighbors=10,\n",
    "          use_destination_embedding_in_message=False,\n",
    "          use_source_embedding_in_message=False,\n",
    "          dyrep=False)\n",
    "tgn.load_state_dict(torch.load(\"out/tgn.pth\"))\n",
    "tgn.to(device)"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:03:46.896530Z",
     "start_time": "2024-03-15T02:03:45.989443Z"
    }
   },
   "id": "initial_id",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "src_embedding, dst_embedding, neg_embedding = tgn.compute_temporal_embeddings(\n",
    "    source_nodes=sources,\n",
    "    destination_nodes=destinations,\n",
    "    negative_nodes=negatives,\n",
    "    edge_times=timestamps,\n",
    "    edge_idxs=edge_idxs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T01:12:04.620414Z",
     "start_time": "2024-03-15T01:12:04.390858Z"
    }
   },
   "id": "687116248d1d56a8",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergiorivera/DataspellProjects/popularity-song-gnn/venv/lib/python3.10/site-packages/numpy/lib/function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "/Users/sergiorivera/DataspellProjects/popularity-song-gnn/venv/lib/python3.10/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n",
      "/Users/sergiorivera/DataspellProjects/popularity-song-gnn/venv/lib/python3.10/site-packages/numpy/lib/function_base.py:2889: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/Users/sergiorivera/DataspellProjects/popularity-song-gnn/venv/lib/python3.10/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Users/sergiorivera/DataspellProjects/popularity-song-gnn/venv/lib/python3.10/site-packages/numpy/lib/function_base.py:2748: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    }
   ],
   "source": [
    "from utils.edges import create_temporal_edges_with_ix, save_temporal_edges\n",
    "\n",
    "parsed = np.load(\"data/parsed.npy\", allow_pickle=True).item()\n",
    "eeg_array = parsed[\"eeg_array\"]\n",
    "track_size = eeg_array.shape[-1]\n",
    "sep_time = 2000\n",
    "t_total = 4742624  # latest timestamp\n",
    "for i in range(len(eeg_array)):\n",
    "    edges = create_temporal_edges_with_ix(eeg_array, i, n_splits=12, corr_threshold=0.9, t_gap=t_total)\n",
    "    save_temporal_edges(f\"eeg_data_{i}\", edges)\n",
    "    t_total += track_size + sep_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T01:18:04.492009Z",
     "start_time": "2024-03-15T01:17:58.776154Z"
    }
   },
   "id": "f7be8b471d1974e7",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from utils.data import process\n",
    "\n",
    "for i in range(len(eeg_array)):\n",
    "    process(f\"eeg_data_{i}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T01:18:28.376742Z",
     "start_time": "2024-03-15T01:18:26.079958Z"
    }
   },
   "id": "2652a102621fe6d",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(len(eeg_array)):\n",
    "    dataset_name = f\"eeg_data_{i}\"\n",
    "    \n",
    "    graph_df = pd.read_csv(f\"out/{dataset_name}.csv\")\n",
    "    sources = graph_df.u.values\n",
    "    destinations = graph_df.i.values\n",
    "    edge_idxs = graph_df.idx.values\n",
    "    labels = graph_df.label.values\n",
    "    timestamps = graph_df.ts.values\n",
    "    \n",
    "    data = Data(sources, destinations, timestamps, edge_idxs, labels)\n",
    "    \n",
    "    neighbor_finder = get_neighbor_finder(data, uniform=False)\n",
    "    random_sampler = RandEdgeSampler(sources, destinations)\n",
    "    size = len(sources)\n",
    "    _, negatives = random_sampler.sample(size)\n",
    "    \n",
    "    node_features = np.load(f\"out/{dataset_name}_node.npy\")\n",
    "    edge_features = np.load(f\"out/{dataset_name}_edge.npy\")\n",
    "    \n",
    "    src_embedding, dst_embedding, neg_embedding = tgn.compute_temporal_embeddings(\n",
    "        source_nodes=sources,\n",
    "        destination_nodes=destinations,\n",
    "        negative_nodes=negatives,\n",
    "        edge_times=timestamps,\n",
    "        edge_idxs=edge_idxs,\n",
    "    )\n",
    "    \n",
    "    save_dict = {\n",
    "        \"src_embedding\": src_embedding,\n",
    "        \"dst_embedding\": dst_embedding,\n",
    "        \"neg_embedding\": neg_embedding,\n",
    "    }\n",
    "    \n",
    "    np.save(f\"out/{dataset_name}_embedding.npy\", save_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T01:53:13.346188Z",
     "start_time": "2024-03-15T01:18:52.808330Z"
    }
   },
   "id": "7537bc22377ede12",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([84, 172])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dict = np.load(\"out/eeg_data_0_embedding.npy\", allow_pickle=True).item()\n",
    "loaded_dict[\"src_embedding\"].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:03:56.162769Z",
     "start_time": "2024-03-15T02:03:56.155137Z"
    }
   },
   "id": "7970c6a4c9debc4",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def decode_embedding(path):\n",
    "    loaded_dict = np.load(path, allow_pickle=True).item()\n",
    "    n_samples = len(loaded_dict[\"src_embedding\"])\n",
    "    return tgn.affinity_score(torch.cat([loaded_dict[\"src_embedding\"], loaded_dict[\"src_embedding\"]], dim=0), torch.cat([loaded_dict[\"dst_embedding\"], loaded_dict[\"neg_embedding\"]])).squeeze(dim=0)[:n_samples]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:08:39.943907Z",
     "start_time": "2024-03-15T02:08:39.938680Z"
    }
   },
   "id": "f0a9716e645f0591",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7a8f033e809969e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, max_sequence_length, indices=None):\n",
    "        self.tabular_data = pd.read_csv(csv_file)\n",
    "        if indices is not None:\n",
    "            self.tabular_data = self.tabular_data.iloc[indices]\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tabular_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.tabular_data.iloc[idx, 2:].to_numpy().astype(np.float32)\n",
    "        features = torch.tensor(row, dtype=torch.float32)\n",
    "        \n",
    "        sequence = decode_embedding(f\"out/eeg_data_{idx}_embedding.npy\").detach().cpu()[0]\n",
    "        padded_sequence = np.zeros(self.max_sequence_length)\n",
    "        padded_sequence[:len(sequence)] = sequence[:self.max_sequence_length]\n",
    "        padded_sequence = torch.tensor(padded_sequence, dtype=torch.float32)\n",
    "        \n",
    "        label = self.tabular_data.iloc[idx, 1]\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        return features, padded_sequence, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:26:18.123971Z",
     "start_time": "2024-03-15T02:26:18.120190Z"
    }
   },
   "id": "e9001fe213cb35f4",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "N = len(pd.read_csv(\"data/features.csv\"))\n",
    "indices = list(range(N))\n",
    "train_indices, test_indices, _, _ = train_test_split(indices, indices, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    csv_file=\"data/features.csv\",\n",
    "    max_sequence_length=10000,\n",
    "    indices=train_indices\n",
    ")\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    csv_file=\"data/features.csv\",\n",
    "    max_sequence_length=10000,\n",
    "    indices=test_indices\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:26:19.666628Z",
     "start_time": "2024-03-15T02:26:19.659232Z"
    }
   },
   "id": "f241083d7a8188e3",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HybridModel, self).__init__()\n",
    "        self.sequence_conv = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.sequence_pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size + 32, 128)  # 32 from sequence processing + features\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x_tabular, x_sequence):\n",
    "        x_sequence = x_sequence.unsqueeze(1)\n",
    "        x_sequence = F.relu(self.sequence_conv(x_sequence))\n",
    "        x_sequence = self.sequence_pool(x_sequence)\n",
    "        x_sequence = x_sequence.view(x_sequence.size(0), -1)\n",
    "        \n",
    "        x_combined = torch.cat((x_tabular, x_sequence), dim=1)\n",
    "        \n",
    "        x = self.relu(self.fc1(x_combined))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:26:20.327869Z",
     "start_time": "2024-03-15T02:26:20.318702Z"
    }
   },
   "id": "a29caba641832643",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2994.8486328125\n",
      "Epoch 2, Loss: 1324.1185302734375\n",
      "Epoch 3, Loss: 1067.29296875\n",
      "Epoch 4, Loss: 613.8297729492188\n",
      "Epoch 5, Loss: 598.9256591796875\n",
      "Epoch 6, Loss: 825.1688232421875\n",
      "Epoch 7, Loss: 405.662841796875\n",
      "Epoch 8, Loss: 429.1819152832031\n",
      "Epoch 9, Loss: 703.5924682617188\n",
      "Epoch 10, Loss: 564.7113647460938\n",
      "Epoch 11, Loss: 373.25811767578125\n",
      "Epoch 12, Loss: 520.2412719726562\n",
      "Epoch 13, Loss: 645.7669067382812\n",
      "Epoch 14, Loss: 750.3186645507812\n",
      "Epoch 15, Loss: 458.4432678222656\n",
      "Epoch 16, Loss: 504.0113525390625\n",
      "Epoch 17, Loss: 400.9752502441406\n",
      "Epoch 18, Loss: 451.33294677734375\n",
      "Epoch 19, Loss: 643.7055053710938\n",
      "Epoch 20, Loss: 530.3818969726562\n",
      "Epoch 21, Loss: 305.5623474121094\n",
      "Epoch 22, Loss: 547.8779907226562\n",
      "Epoch 23, Loss: 421.8311462402344\n",
      "Epoch 24, Loss: 696.3097534179688\n",
      "Epoch 25, Loss: 824.0570068359375\n",
      "Epoch 26, Loss: 416.90814208984375\n",
      "Epoch 27, Loss: 506.1241149902344\n",
      "Epoch 28, Loss: 493.1675109863281\n",
      "Epoch 29, Loss: 340.216552734375\n",
      "Epoch 30, Loss: 249.2373046875\n",
      "Epoch 31, Loss: 542.0223999023438\n",
      "Epoch 32, Loss: 617.585205078125\n",
      "Epoch 33, Loss: 425.9391784667969\n",
      "Epoch 34, Loss: 619.7771606445312\n",
      "Epoch 35, Loss: 257.73089599609375\n",
      "Epoch 36, Loss: 315.29168701171875\n",
      "Epoch 37, Loss: 372.9297790527344\n",
      "Epoch 38, Loss: 444.0215759277344\n",
      "Epoch 39, Loss: 443.5299072265625\n",
      "Epoch 40, Loss: 393.0450134277344\n",
      "Epoch 41, Loss: 429.299560546875\n",
      "Epoch 42, Loss: 481.0423278808594\n",
      "Epoch 43, Loss: 388.8330383300781\n",
      "Epoch 44, Loss: 404.6979064941406\n",
      "Epoch 45, Loss: 247.06106567382812\n",
      "Epoch 46, Loss: 316.1647033691406\n",
      "Epoch 47, Loss: 262.1522521972656\n",
      "Epoch 48, Loss: 669.4321899414062\n",
      "Epoch 49, Loss: 329.5544738769531\n",
      "Epoch 50, Loss: 236.51885986328125\n",
      "Epoch 51, Loss: 323.3945007324219\n",
      "Epoch 52, Loss: 328.0692443847656\n",
      "Epoch 53, Loss: 192.23118591308594\n",
      "Epoch 54, Loss: 429.6556091308594\n",
      "Epoch 55, Loss: 502.06744384765625\n",
      "Epoch 56, Loss: 404.8565979003906\n",
      "Epoch 57, Loss: 450.2543640136719\n",
      "Epoch 58, Loss: 441.90325927734375\n",
      "Epoch 59, Loss: 200.8765106201172\n",
      "Epoch 60, Loss: 397.37933349609375\n",
      "Epoch 61, Loss: 389.7178649902344\n",
      "Epoch 62, Loss: 292.0333557128906\n",
      "Epoch 63, Loss: 258.9795837402344\n",
      "Epoch 64, Loss: 305.7157897949219\n",
      "Epoch 65, Loss: 525.0975952148438\n",
      "Epoch 66, Loss: 226.26434326171875\n",
      "Epoch 67, Loss: 345.04461669921875\n",
      "Epoch 68, Loss: 293.4848327636719\n",
      "Epoch 69, Loss: 488.6620178222656\n",
      "Epoch 70, Loss: 438.7508239746094\n",
      "Epoch 71, Loss: 551.5442504882812\n",
      "Epoch 72, Loss: 397.004638671875\n",
      "Epoch 73, Loss: 448.4363708496094\n",
      "Epoch 74, Loss: 376.3951721191406\n",
      "Epoch 75, Loss: 385.7200012207031\n",
      "Epoch 76, Loss: 365.30377197265625\n",
      "Epoch 77, Loss: 243.8775634765625\n",
      "Epoch 78, Loss: 362.480712890625\n",
      "Epoch 79, Loss: 204.4628143310547\n",
      "Epoch 80, Loss: 242.0834197998047\n",
      "Epoch 81, Loss: 344.4582824707031\n",
      "Epoch 82, Loss: 524.4493408203125\n",
      "Epoch 83, Loss: 352.2701416015625\n",
      "Epoch 84, Loss: 244.8916015625\n",
      "Epoch 85, Loss: 283.65618896484375\n",
      "Epoch 86, Loss: 410.6481018066406\n",
      "Epoch 87, Loss: 385.8406982421875\n",
      "Epoch 88, Loss: 285.36346435546875\n",
      "Epoch 89, Loss: 180.5414276123047\n",
      "Epoch 90, Loss: 516.42236328125\n",
      "Epoch 91, Loss: 206.8988037109375\n",
      "Epoch 92, Loss: 575.8325805664062\n",
      "Epoch 93, Loss: 603.0480346679688\n",
      "Epoch 94, Loss: 516.842529296875\n",
      "Epoch 95, Loss: 184.55355834960938\n",
      "Epoch 96, Loss: 336.4232482910156\n",
      "Epoch 97, Loss: 260.574462890625\n",
      "Epoch 98, Loss: 311.7185974121094\n",
      "Epoch 99, Loss: 452.5719299316406\n",
      "Epoch 100, Loss: 497.58929443359375\n"
     ]
    }
   ],
   "source": [
    "def train(model, criterion, optimizer, train_loader, epochs=100):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (features, sequences, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Model forward pass\n",
    "            output = model(features, sequences)\n",
    "            \n",
    "            loss = criterion(output.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "model = Regressor(6)  # 6 is number of feature fields\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "losses = train(model, criterion, optimizer, train_loader, epochs=100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:50:38.342657Z",
     "start_time": "2024-03-15T02:44:57.953479Z"
    }
   },
   "id": "254218f1957ee024",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 325.8663\n"
     ]
    },
    {
     "data": {
      "text/plain": "325.8662872314453"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, sequences, labels) in enumerate(test_loader):\n",
    "            output = model(features, sequences)\n",
    "            loss = criterion(output.squeeze(), labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    print(f'Test Loss: {average_loss:.4f}')\n",
    "    return average_loss\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "evaluate(model, criterion, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T02:58:51.601919Z",
     "start_time": "2024-03-15T02:58:49.551393Z"
    }
   },
   "id": "7b43106b9c942a9c",
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
